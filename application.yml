openai:
    key: xx-xxxxxxxx    # *REQUIRED* your API key
    organization: xx-xxxxxxxx   # your organization key
    textCompletion:
        model: "text-davinci-003" # the model used for the completion
        suffix: "end" # the suffix that comes after a completion of inserted text
        maxTokens: 4096 # the maximum number of tokens generated by ai as a response completion.
        temperature: 1  # 0~2 decimal. degree of randomness in the output. more randomness with bigger value
        topP: 1 # another parameter controlling the randomness. OpenAi suggests not setting both temperature and topP simultaneously
        n: 1    # Number of candidate completions generated for a same prompt request
        stream: false   # *YET_NOT_SUPPORTED* if the completion responds as a Server-Sent-Event stream
        logprobs: 0 # Include the log probabilities on the most likely tokens, as well the chosen tokens.
        echo: false # Echo back the prompt in addition to the completion
        stop: ["bye", "ok"] # a list of at most 4 sequences where the API will stop generating further tokens.
        presencePenalty: 0  # -2~2 decemal. official explanation: https://platform.openai.com/docs/api-reference/parameter-details
        frequencyPenalty: 0 # -2~2 decemal.  official explanation: https://platform.openai.com/docs/api-reference/parameter-details
        bestOf: 0  # Generates best_of completions server-side and returns the best (the one with the highest log probability per token).
        logitBias: # -100~100 decimal. the likelihood of each specific token appears in the completion
            1965: 1.0   # the logit bias of token 1964 is 1.0.
            2023: -0.5  #  token 2023 is more unlikely to appear in the completion since its logit bias is negative.
        user: "cena"  # the user of the completion request
    chatCompletion:
        model: "gpt-3.5-turbo"    # the model used for the completion
        temperature: 1  # 0~2 decimal. degree of randomness in the output. more randomness with bigger value
        topP: 1 # another parameter controlling the randomness. OpenAi suggests not setting both temperature and topP simultaneously
        n: 1    # Number of candidate completions generated for a same prompt request
        stream: false   # *YET_NOT_SUPPORTED* if the completion responds as a Server-Sent-Event stream
        stop: ["bye", "ok"] # a list of at most 4 sequences where the API will stop generating further tokens.
        maxPromptToken: 3000    # the maximum number of tokens sourced from the preceding context that can be used for a request prompt.
        maxCompletionToken: 4096    # the maximum number of tokens generated by ai as a response completion.
        presencePenalty: 0  # -2~2 decemal. official explanation: https://platform.openai.com/docs/api-reference/parameter-details
        frequencyPenalty: 0 # -2~2 decemal.  official explanation: https://platform.openai.com/docs/api-reference/parameter-details
        logitBias: # -100~100 decimal. the likelihood of each specific token appears in the completion
            1965: 1.0   # the logit bias of token 1964 is 1.0.
            2023: -0.5  #  token 2023 is more unlikely to appear in the completion since its logit bias is negative.
        user: "cena"  # the user of the completion request
    edit:
        model: "text-davinci-edit-001"    # the model used for the task
        temperature: 1  # 0~2 decimal. degree of randomness in the output. more randomness with bigger value
        topP: 1 # another parameter controlling the randomness. OpenAi suggests not setting both temperature and topP simultaneously
        n: 1    # Number of candidate editss generated for a request
    moderation:
        model: "text-moderation-latest" # the model used for the moderation
    imageGeneration:
        n: 1    # number of candidate images generated for a request
        size: "1024x1024"  # the size of the image.
        response_format: "url"  # the format of the image in the response.
        user: "cena"  # the user of the completion request
    imageEdit:
        n: 1    # number of candidate images generated for a request
        size: "1024x1024"  # the size of the image.
        response_format: "url"  # the format of the image in the response.
        user: "cena"  # the user of the completion request
    imageVariation:
        n: 1    # number of candidate images generated for a request
        size: "1024x1024"  # the size of the image.
        response_format: "url"  # the format of the image in the response.
        user: "cena"  # the user of the completion request
    embedding:
        model: "text-embedding-ada-002" # the model for the task
        user: "cena"  # the user of the completion request
    audioTranscription:
        model: "whisper-1"  # the model for audio tasks. There is no alternative choices although OpenAi provided this parameter.
        responseFormat: "json"  # the format of the generated transcription text.
        language: "en" # language in ISO-639-1 format.
        temperature: 0 # 0~1 decimal. degree of randomness in the output. more randomness with bigger value.
    audioTranslation:
        model: "whisper-1"  # the model for audio tasks. There is no alternative choices although OpenAi provided this parameter.
        responseFormat: "json"  # the format of the generated transcription text.
        temperature: 0 # 0~1 decimal. degree of randomness in the output. more randomness with bigger value.
    fineTune: # the following are the request parameters for the create fine tune API. Considering its professionalism, please visit https://platform.openai.com/docs/api-reference/fine-tunes/create for full explanation.
        model: "curie"
        nEpochs: 4
        batchSize: 10
        learningRateMultiplier: 0.02
        promptLossWeight: 0.01
        computeClassificationMetrics: false
        classificationNClasses: 3
        classificationPositiveClass: "positive"
        classificationBetas: [0.1, 0.1]
        suffix: "custom-model-name"