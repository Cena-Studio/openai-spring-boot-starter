openai:
    key: xx-xxxxxxxx    # *REQUIRED* your API key
    organization: xx-xxxxxxxx   # your organization key
    chatCompletion:
        model: gpt-3.5-turbo    # the model used for the completion
        temperature: 1  # 0~2 decimal. degree of randomness in the output. more randomness with bigger value
        topP: 1 # another parameter controlling the randomness. OpenAi suggests not setting both temperature and topP simultaneously
        n: 1    # Number of candidate completions generated for a same prompt request
        stream: false   # *YET_NOT_SUPPORTED* if the completion responds as a Server-Sent-Event stream
        stop: ["bye", "ok"] # a list of at most 4 sequences where the API will stop generating further tokens.
        maxPromptToken: 3000    # the maximum number of tokens sourced from the preceding context that can be used for a request prompt.
        maxCompletionToken: 4096    # the maximum number of tokens generated by ai as a response completion.
        presencePenalty: 0 # *YET_NOT_SUPPORTED* -2~2 decemal.
        frequencyPenalty: 0 # *YET_NOT_SUPPORTED* -2~2 decemal.
        logitBias: # -100~100 decimal. the likelihood of each specific token appears in the completion
            1965: 1.0   # the logit bias of token 1964 is 1.0.
            2023: -0.5  #  token 2023 is more unlikely to appear in the completion since its logit bias is negative.
        user: cena  # the user of the completion request